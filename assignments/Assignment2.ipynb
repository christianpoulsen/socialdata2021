{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2.\n",
    "\n",
    "## Formalia:\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/suneman/socialdata2021/wiki/Assignment-1-and-2) carefully before proceeding. This page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "_If you fail to follow these simple instructions, it will negatively impact your grade!_\n",
    "\n",
    "**Due date and time**: The assignment is due on Monday April 5th, 2021 at 23:55. Hand in your files via [`http://peergrade.io`](http://peergrade.io/).\n",
    "\n",
    "**Peergrading date and time**: _Remember that after handing in you have a week to evaluate a few assignments written by other members of the class_. Thus, the peer evaluations are due on Monday April 12th, 2021 at 23:55. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Questions to text and lectures.\n",
    "\n",
    "A) Please answer my questions to the Segal and Heer paper we read during lecture 7 and 8.\n",
    "\n",
    "* What is the *Oxford English Dictionary's* defintion of a narrative?\n",
    "* What is your favorite visualization among the examples in section 3? Explain why in a few words.\n",
    "* What's the point of Figure 7?\n",
    "* Use Figure 7 to find the most common design choice within each category for the Visual narrative and Narrative structure (the categories within visual narrative are 'visual structuring', 'highlighting', etc).\n",
    "* Check out Figure 8 and section 4.3. What is your favorite genre of narrative visualization? Why? What is your least favorite genre? Why?\n",
    "\n",
    "\n",
    "B) Also please answer the questions to my talk on [explanatory data visualization](https://www.youtube.com/watch?v=yHKYMGwefso)\n",
    "\n",
    "* What are the three key elements to keep in mind when you design an explanatory visualization?\n",
    "* In the video I talk about (1) *overview first*,  (2) *zoom and filter*,  (3) *details on demand*. \n",
    "  - Go online and find a visualization that follows these principles (don't use one from the video). \n",
    "  - Explain how it does achieves (1)-(3). It might be useful to use screenshots to illustrate your explanation.\n",
    "* Explain in your own words: How is explanatory data analysis different from exploratory data analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Random forest and weather\n",
    "\n",
    "The aim here is to recreate the work you did in Part 1-3 of the Week 7 lecture. I've phrased things differently relative to the exercise to make the purpose more clear. \n",
    "\n",
    "Part 2A: Random forest binary classification. \n",
    "* Using the and instructions and material from Week 7, build a *random forest* classifier to distinguish between two types (you choose) of crime using on spatio-temporal (where/when) features of data describing the two crimes. When you're done, you should be able to give the classifier a place and a time, and it should tell you which of the two  types of crime happened there.\n",
    "  - Explain about your choices for training/test data, features, and encoding. (You decide how to present your results, but here are some example topics to consider: Did you balance the training data? What are the pros/cons of balancing? Do you think your model is overfitting? Did you choose to do cross-validation? Which specific features did you end up using? Why? Which features (if any) did you one-hot encode? Why ... or why not?))\n",
    "  - Report accuracy. Discuss the model performance.\n",
    "  \n",
    "  \n",
    "Part 2B: Info from weather features.\n",
    "* Now add features from weather data to your random forest. \n",
    "  - Report accuracy. \n",
    "  - Discuss how the model performance changes relative to the version with no weather data.\n",
    "  - Discuss what you have learned about crime from including weather data in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "raw_crimes = pd.read_csv(\"../incidents.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from datetime import datetime\n",
    "\n",
    "focuscrimes = [\"BURGLARY\", \"FRAUD\"]\n",
    "\n",
    "crimes = raw_crimes[raw_crimes[\"Category\"].isin(focuscrimes)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "cat = \"category\"\n",
    "dt = \"datetime\"\n",
    "year = \"year\"\n",
    "month = \"month\"\n",
    "day = \"day\"\n",
    "hour = \"hour\"\n",
    "hour_of_month = \"hour_of_month\"\n",
    "hour_of_week = \"hour_of_week\"\n",
    "day_of_week = \"dayofweek\"\n",
    "pddistrict = \"pddistrict\"\n",
    "\n",
    "crimes.columns = crimes.columns.str.lower()\n",
    "crimes[dt_feature] = pd.to_datetime(crimes[\"date\"] + \" \" + crimes[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes[year] = crimes[dt].dt.year\n",
    "crimes[month] = crimes[dt].dt.month\n",
    "crimes[day] = crimes[dt].dt.day\n",
    "crimes[hour] = crimes[dt].dt.hour\n",
    "\n",
    "crimes[hour_of_month] = crimes.apply(lambda row: row[dt].day * 24 + row[hour], axis=1)\n",
    "crimes[hour_of_week] = crimes.apply(lambda row: row[dt].dayofweek * 24 + row[hour], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35912, 42)\n",
      "(16703, 42)\n"
     ]
    }
   ],
   "source": [
    "crimes_in_range = crimes[crimes[year].between(2012, 2017, inclusive=True)]\n",
    "burglary = crimes_in_range[crimes_in_range[cat].isin([focuscrimes[0]])]\n",
    "fraud = crimes_in_range[crimes_in_range[cat].isin([focuscrimes[1]])]\n",
    "\n",
    "print(burglary.shape)\n",
    "print(fraud.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 15000\n",
    "\n",
    "# Create balanced data set\n",
    "type1 = burglary.sample(sample_size)\n",
    "type2 = fraud.sample(sample_size)\n",
    "\n",
    "crime_df = pd.concat([type1, type2], ignore_index=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (22500, 19)\n",
      "Training Labels Shape: (22500,)\n",
      "Testing Features Shape: (7500, 19)\n",
      "Testing Labels Shape: (7500,)\n"
     ]
    }
   ],
   "source": [
    "crime_features = [cat, day_of_week, month, hour, pddistrict]\n",
    "crime_dummies = [day_of_week, pddistrict]\n",
    "\n",
    "features = crime_df[crime_features].copy()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "features[cat] = le.fit_transform(features[cat])\n",
    "\n",
    "# One-hot encode the categorical data\n",
    "features = pd.get_dummies(features, columns=crime_dummies)\n",
    "\n",
    "# Labels will be the values we want to predict\n",
    "labels = np.array(features[cat])\n",
    "\n",
    "# We remove the labels from the crime dataframe to get all the values we need for the features\n",
    "features = features.drop(cat, axis=1)\n",
    "\n",
    "# We save the feature names for later\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "# Convert the dataframe to a numpy array so we can work with the features\n",
    "features = np.array(features)\n",
    "\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Max Average Tree Depth: 29.757575757575758\n",
      "Training\n",
      "Mean Absolute Error: 0.17 degrees.\n",
      "Accuracy:  82.91111111111111 %\n",
      "\n",
      "Testing\n",
      "Mean Absolute Error: 0.42 degrees.\n",
      "TeAccuracy:  58.45333333333333 %\n",
      "\n",
      "Max Depth 3\n",
      "Training\n",
      "Mean Absolute Error: 0.4 degrees.\n",
      "Accuracy:  60.04888888888888 %\n",
      "\n",
      "Testing\n",
      "Mean Absolute Error: 0.4 degrees.\n",
      "TeAccuracy:  60.06666666666667 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "no_max_classifier = RandomForestClassifier(n_estimators=99, random_state=42)\n",
    "no_max_classifier.fit(train_features, train_labels)\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=99, random_state=42, max_depth=3)\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "print(\"No Max Average Tree Depth:\", np.mean([estimator.get_depth() for estimator in no_max_classifier.estimators_]))\n",
    "\n",
    "no_max_train_predictions = no_max_classifier.predict(train_features)\n",
    "print(\"Training\")\n",
    "print('Mean Absolute Error:', round(mean_absolute_error(train_labels, no_max_train_predictions), 2), 'degrees.')\n",
    "print(\"Accuracy: \", 100 * no_max_classifier.score(train_features, train_labels), \"%\\n\")\n",
    "\n",
    "no_max_predictions = no_max_classifier.predict(test_features)\n",
    "print(\"Testing\")\n",
    "print('Mean Absolute Error:', round(mean_absolute_error(test_labels, no_max_predictions), 2), 'degrees.')\n",
    "print(\"TeAccuracy: \", 100 * no_max_classifier.score(test_features, test_labels), \"%\\n\")\n",
    "\n",
    "\n",
    "print(\"Max Depth 3\")\n",
    "\n",
    "train_predictions = classifier.predict(train_features)\n",
    "print(\"Training\")\n",
    "print('Mean Absolute Error:', round(mean_absolute_error(train_labels, train_predictions), 2), 'degrees.')\n",
    "print(\"Accuracy: \", 100 * classifier.score(train_features, train_labels), \"%\\n\")\n",
    "\n",
    "predictions = classifier.predict(test_features)\n",
    "print(\"Testing\")\n",
    "print('Mean Absolute Error:', round(mean_absolute_error(test_labels, predictions), 2), 'degrees.')\n",
    "print(\"Accuracy: \", 100 * classifier.score(test_features, test_labels), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2A\n",
    "\n",
    "**Did you balance the training data? What are the pros/cons of balancing?**\n",
    "\n",
    "The dataset is balanced with 20000 randomly picked samples from each crime category, as to ensure the crime are distributed equally over time with no favor of one over the other.\n",
    "\n",
    "**Do you think your model is overfitting?**\n",
    "\n",
    "Initially where the classifier had no maximum depth, the training accuracy was near 89% where test accuracy was at 52-53%. Together with a avg. tree depth of around 45 of a dataset with 18 feautures, it seems safe to assume that the model was overfitting, as it clearly shows it did not generalize well from the training data to the testing data.\n",
    "\n",
    "However, with a maximum depth of 3, a higher accuracy is reached but with a drastical smaller tree size, which could indicate a better fitted model.\n",
    "\n",
    "**Did you choose to do cross-validation?**\n",
    "\n",
    "To error estimate the classifier, the Holdout Method is used by creating training and testing/validation datasets. The testing datasets are then used to calculate the mean accuracy of the classifier.\n",
    "\n",
    "**Which specific features did you end up using? Why?**\n",
    "\n",
    "The features used are \"DayOfWeek\", \"Date\", \"Time\", and \"PdDistrict\", because they tell something about the time and place of the crime.\n",
    "\n",
    "**Which features (if any) did you one-hot encode? Why ... or why not?))**\n",
    "\n",
    "The features to be one-hot encoded was \"DayOfWeek\" and \"PdDistrict\", where the crime category was just label encoded. Both \"DayOfWeek\" and \"PdDistrict\" includes categorical variables that should be converted to binary data which the machine can understand without preferring one over the other, why Pandas' get_dummies function is used.\n",
    "\n",
    "Because the crime category is what should be predicted, these are not converted to binary data, but are just given a numeric representation using Sklearn's LabelEncoder.\n",
    "\n",
    "The \"Date\" and \"Time\" features are also kind of included. When the raw crime data is loaded into a dataframe, the columns are just treated as strings. We want to use them to determine how time influceses the crimes. To make the machine understand this, however, the columns are merged together to a datetime column \"Date_Time\" where the dates are converted to their ordinal numeric values.\n",
    "At the time this seemed smart, but after doing some thinking, it would probably have been better to split the date times up into something like; year, month, day, hour, minute or something, as humans, and therefor crimes, follow more patterns of our gregorian calendar rather than a UNIX timestamp...\n",
    "\n",
    "**Report accuracy. Discuss the model performance.**\n",
    "\n",
    "Well, the accuracy is around 57% for the Random Forest classifier, an 14 % better accuracy of the baseline of 50/50. This is probably not good enough for any practical application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>weather</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>pressure</th>\n",
       "      <th>datetime</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>hour_of_month</th>\n",
       "      <th>hour_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-10-01T13:00:00.000Z</td>\n",
       "      <td>16.330000</td>\n",
       "      <td>88.0</td>\n",
       "      <td>light rain</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>2012-10-01 13:00:00+00:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>37</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-10-01T14:00:00.000Z</td>\n",
       "      <td>16.324993</td>\n",
       "      <td>87.0</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>2.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>2012-10-01 14:00:00+00:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-10-01T15:00:00.000Z</td>\n",
       "      <td>16.310618</td>\n",
       "      <td>86.0</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>2.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>2012-10-01 15:00:00+00:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-10-01T16:00:00.000Z</td>\n",
       "      <td>16.296243</td>\n",
       "      <td>85.0</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>2.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>2012-10-01 16:00:00+00:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-10-01T17:00:00.000Z</td>\n",
       "      <td>16.281869</td>\n",
       "      <td>84.0</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>2012-10-01 17:00:00+00:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>41</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  temperature  humidity       weather  wind_speed  \\\n",
       "0  2012-10-01T13:00:00.000Z    16.330000      88.0    light rain         2.0   \n",
       "1  2012-10-01T14:00:00.000Z    16.324993      87.0  sky is clear         2.0   \n",
       "2  2012-10-01T15:00:00.000Z    16.310618      86.0  sky is clear         2.0   \n",
       "3  2012-10-01T16:00:00.000Z    16.296243      85.0  sky is clear         2.0   \n",
       "4  2012-10-01T17:00:00.000Z    16.281869      84.0  sky is clear         2.0   \n",
       "\n",
       "   wind_direction  pressure                  datetime  year  month  day  hour  \\\n",
       "0           150.0    1009.0 2012-10-01 13:00:00+00:00  2012     10    1    13   \n",
       "1           147.0    1009.0 2012-10-01 14:00:00+00:00  2012     10    1    14   \n",
       "2           141.0    1009.0 2012-10-01 15:00:00+00:00  2012     10    1    15   \n",
       "3           135.0    1009.0 2012-10-01 16:00:00+00:00  2012     10    1    16   \n",
       "4           129.0    1009.0 2012-10-01 17:00:00+00:00  2012     10    1    17   \n",
       "\n",
       "   hour_of_month  hour_of_week  \n",
       "0             37            13  \n",
       "1             38            14  \n",
       "2             39            15  \n",
       "3             40            16  \n",
       "4             41            17  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pd.read_csv(\"../weather_data.csv\")\n",
    "\n",
    "# Format date and time for easy processing and training\n",
    "weather[dt] = pd.to_datetime(weather[\"date\"])\n",
    "weather[year] = weather[dt].dt.year\n",
    "weather[month] = weather[dt].dt.month\n",
    "weather[day] = weather[dt].dt.day\n",
    "weather[hour] = weather[dt].dt.hour\n",
    "\n",
    "weather[hour_of_month] = weather.apply(lambda row: row[dt].day * 24 + row[hour], axis=1)\n",
    "weather[hour_of_week] = weather.apply(lambda row: row[dt].dayofweek * 24 + row[hour], axis=1)\n",
    "\n",
    "# Fix naming\n",
    "weather.head()\n",
    "\n",
    "# One-hot encode the categorical data\n",
    "# weather_df = pd.get_dummies(weather, columns=[\"Weather\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge the weather and crime dataframes together!\n",
    "\n",
    "merged_df = pd.merge(crime_df, weather, how=\"left\", on=[year, month, day, hour, hour_of_month, hour_of_week])\n",
    "merged_df.dropna()\n",
    "\n",
    "weather_features = [\"weather\", \"humidity\", \"temperature\", \"pressure\", \"wind_speed\"]\n",
    "weather_dummies = [\"weather\"]\n",
    "\n",
    "concatted_features = np.concatenate([weather_features, crime_features])\n",
    "concatted_dummies = np.concatenate([weather_dummies, crime_dummies])\n",
    "\n",
    "merged_features = merged_df[concatted_features].copy()\n",
    "merged_features = merged_features.dropna()\n",
    "\n",
    "# Label encode the categories\n",
    "merged_features[cat] = le.fit_transform(merged_features[cat])\n",
    "\n",
    "# One-hot encode the categorical data\n",
    "merged_features = pd.get_dummies(merged_features, columns=concatted_dummies)\n",
    "\n",
    "# Labels will be the values we want to predict\n",
    "merged_labels = np.array(merged_features[cat])\n",
    "\n",
    "# We remove the labels from the crime dataframe to get all the values we need for the features\n",
    "merged_features = merged_features.drop(cat, axis=1)\n",
    "\n",
    "# We save the feature names for later\n",
    "merged_feature_list = list(merged_features.columns)\n",
    "\n",
    "# Convert the dataframe to a numpy array so we can work with the features\n",
    "merged_features = np.array(merged_features)\n",
    "\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(merged_features, labels, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Mean Absolute Error: 0.35 degrees.\n",
      "Accuracy:  64.99607227022781 %\n",
      "\n",
      "Testing\n",
      "Mean Absolute Error: 0.38 degrees.\n",
      "TeAccuracy:  61.91673212882953 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=99, random_state=42, max_depth=7)\n",
    "clf.fit(train_features, train_labels)\n",
    "\n",
    "preds = clf.predict(train_features)\n",
    "print(\"Training\")\n",
    "print('Mean Absolute Error:', round(mean_absolute_error(train_labels, preds), 2), 'degrees.')\n",
    "print(\"Accuracy: \", 100 * clf.score(train_features, train_labels), \"%\\n\")\n",
    "\n",
    "preds = clf.predict(test_features)\n",
    "print(\"Testing\")\n",
    "print('Mean Absolute Error:', round(mean_absolute_error(test_labels, preds), 2), 'degrees.')\n",
    "print(\"TeAccuracy: \", 100 * clf.score(test_features, test_labels), \"%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2B\n",
    "\n",
    "**Report accuracy**\n",
    "\n",
    "Well, the accuracy haven't really improved much.\n",
    "\n",
    "**Discuss how the model performance changes relative to the version with no weather data**\n",
    "\n",
    "Not much?\n",
    "\n",
    "**Discuss what you have learned about crime from including weather data in your model**\n",
    "\n",
    "That weather is probably not a good indicator for predicting crimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data visualization\n",
    "\n",
    "* Create the Bokeh visualization from Part 2 of the Week 8 Lecture, displayed in a beautiful `.gif` below. \n",
    "* Provide nice comments for your code. Don't just use the `# inline comments`, but the full Notebook markdown capabilities and explain what you're doing.\n",
    "\n",
    "![Movie](https://github.com/suneman/socialdataanalysis2020/blob/master/files/week8_1.gif?raw=true \"movie\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
