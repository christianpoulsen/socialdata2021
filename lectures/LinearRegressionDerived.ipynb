{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fundamental-webmaster",
   "metadata": {},
   "source": [
    "## Here is Sune's fun derivation of the closed form solution for the coeffients in linear regression\n",
    "\n",
    "### Setting up the problem\n",
    "\n",
    "Ok. So first, let's introduce some [nomenclature](https://www.merriam-webster.com/dictionary/nomenclature). We want to fit a line of the form $y = ax + b$, where $a$ is the slope and $b$ is the $y$-intercept. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/suneman/socialdataanalysis2019/master/files/sketch1.png\" alt=\"drawing\" width=\"650\"/>\n",
    "\n",
    "Here, the line is shown in red, and we've also given names to all our data. There are $N$ points, and point $i$ has coordinates $(x_i,y_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-evanescence",
   "metadata": {},
   "source": [
    "### So what is it we want to optimize?\n",
    "\n",
    "Now we can think about how we actually implement the best possible fit. We won't just do something *ad hoc* like moving the line around until it looks good. Here, we want to use the power of math to make sure that we actually find the best fit. \n",
    "\n",
    "A promising concept to make the best fit something practical is the *residuals*. For some $x_i$, the residual is the difference between the actual value $y_i$ and the value of the line in that point $ax_i+b$. On the plot the differences look like this (with residuals illustrated in green)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/suneman/socialdataanalysis2019/master/files/sketch3.png\" alt=\"drawing\" width=\"650\"/>\n",
    "\n",
    "Below the plot, I've also sketched the size of the residuals. (Note the connection of the little inset to DAOST Figure 3-5.)\n",
    "\n",
    "**So if we can make the residuals as small as possible, we will find a good fit**. But how do we do that?\n",
    "\n",
    "### The sum of squares\n",
    "Basically the idea is that we want to minimize some function of all the residuals that takes into account their sizes. We can't just add them in a big sum, since they're both positive and negative. If we did that, we would end up with a small sum, as long as we had a roughly equal number of positive/negative residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-dylan",
   "metadata": {},
   "source": [
    "Another candidate for a function that you might come up with now (especially if you think like a programmer), involve minimizing the sum of absolute values of residuals. That might actually work well, but it's non-trivial to work with absolute values from the perspective of the mathematics ([plus there are other good reasons to follow a different route](https://math.stackexchange.com/questions/967883/why-get-the-sum-of-squares-instead-of-the-sum-of-absolute-values) as we do below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-drain",
   "metadata": {},
   "source": [
    "In fact, a lovely way to make sure the residuals are positive, is to simply *square them*. Minimizing the sum of squares also minimizes the sum of absolute values. So now we're good to go. If we call the sum of squares $Q$, the thing we want to make as small as possible is\n",
    "\n",
    "$$Q = \\sum_{i=1}^N \\left(y_i - (ax + b) \\right)^2$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-premiere",
   "metadata": {},
   "source": [
    "**But how the h#ll do we do that?** Is it time for gradient descent or some numerical optimization like that?\n",
    "\n",
    "Most of the time, when we're finding optima of complicated functions, the answer is yes, but in this simple case, the answer is no. \n",
    "\n",
    "Because this is a beautiful and simple problem, we can find an analytical solution using relatively simple methods. (And the happy memory of learning about *that*, is why I felt like going over the solution today ... although I'm starting to regret it a bit ... since it's a lot of work explain all this ðŸ¥´ðŸ¤ª).\n",
    "\n",
    "But there's no point in stopping now. So let's do it!\n",
    "\n",
    "### Minimizing the sum of squares\n",
    "How do we find the minimum of something without coding? Now, you have to think *all the way back* to when you did your high school math. (Ah, those were the days, right.)\n",
    "\n",
    "And yes, you're right. $Q$ will be minimized at the values of $a$ and $b$ for which $\\partial Q / \\partial a = 0$ and $\\partial Q / \\partial b = 0$. So we can simply find those.\n",
    "\n",
    "We start with the $y$-intecept, $b$. Now, we find that by setting the derivative wrt. $b$ equal to zero\n",
    "\n",
    "$$\n",
    "\\frac{ \\partial Q } {\\partial b} = \\sum_{i=1}^N -2 \\left(y_i - b - ax_i\\right)=\n",
    "2\\left(N b + a \\sum_{i=1}^Nx_i - \\sum_{i=1}^Ny_i\\right) = 0.\n",
    "$$\n",
    "\n",
    "We can reorganize this to give us a nice interpretation of $b$\n",
    "\n",
    "$$\n",
    "\\tag{1}\n",
    "b = \\langle y \\rangle - a \\langle x \\rangle,\n",
    "$$\n",
    "\n",
    "where $\\langle x \\rangle = (1/N)\\sum_i x_i$ is the mean value of the $x_i$ and $\\langle y \\rangle = (1/N)\\sum_i x_i$ is the mean value of the $y_i$. \n",
    "\n",
    "**Equation (1) is what you will need later on**, but it's useless without an expression for the slope $a$, so let's find that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-navigator",
   "metadata": {},
   "source": [
    "This time we get\n",
    "\n",
    "$$\n",
    "\\frac{ \\partial Q } {\\partial a} = \\sum_{i=1}^N -2x_i \\left(y_i - b - ax_i\\right)=\n",
    "\\sum_{i=1}^N -2\\left(x_iy_i - bx_i - ax_i^2\\right) = 0.\n",
    "$$\n",
    "\n",
    "We now substitute our solution for $b$ into this equation to yield\n",
    "$$\n",
    "\\sum_{i=1}^N \\left( x_iy_i - x_i \\langle y\\rangle + ax_i\\langle x \\rangle -  a x_i^2   \\right) = 0.\n",
    "$$\n",
    "\n",
    "And we can split this bad-boy into two sums to give us\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\left( x_iy_i - x_i \\langle y\\rangle\\right) - \n",
    "a\\sum_{i=1}^N \\left(x_i^2 - x_i\\langle x \\rangle \\right) = 0,\n",
    "$$\n",
    "\n",
    "where we can isolate $a$ to give us a **final expression for the slope, shown below in Equation (2).**\n",
    "\n",
    "$$\n",
    "\\tag{2}\n",
    "a = \\frac{\\sum_{i=1}^N \\left( x_iy_i - x_i \\langle y\\rangle\\right)}{\\sum_{i=1}^N \\left(x_i^2 - x_i\\langle x \\rangle \\right)}=\n",
    "\\frac{\\sum_{i=1}^N \\left( x_iy_i \\right) - N \\langle x\\rangle\\langle y\\rangle }{\\sum_{i=1}^N\\left( x_i^2 \\right) - N\\langle x\\rangle^2}.\n",
    "$$\n",
    "Aaaand we're done. \n",
    "\n",
    "***Important summary information:*** To use my ramblings above for something constructive, simply start by calculating Equation (2), then plug your answer into Equation (1) and you'll have the best fit line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-advertiser",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
