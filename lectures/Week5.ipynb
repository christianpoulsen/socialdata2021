{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5\n",
    "\n",
    "Phew. You've handed in the assignment. But there's not resting now. We're just hitting out grove, so let's get going!! Much to get through today.\n",
    "\n",
    "## The informal intro\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/jhwuSAlbWHs/0.jpg)](https://www.youtube.com/watch?v=jhwuSAlbWHs)\n",
    "\n",
    "## Feedback on the questionnaire\n",
    "\n",
    "Last week many of you also took the time to fill out a questionnaire. I go over your answers in the video below.\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/K8mDx10BZ44/0.jpg)](https://www.youtube.com/watch?v=K8mDx10BZ44)\n",
    "\n",
    "\n",
    "## The plan for today\n",
    "\n",
    "We continue exploring data and learning about dataviz. The lecture today has 4 parts.\n",
    "* In part 1, there is more lecturing.\n",
    "* In part 2, we talk about exploring data with two variables - we read a bit more.\n",
    "* In part 3, it's a short one about logarithmic plots.\n",
    "* And finally, in part 4, we have fun with linear regression.\n",
    "\n",
    "Ok. Now it's time to get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: More lecturing on dataviz\n",
    "\n",
    "So now we start learning more about the theory of visualization, digging into data encodings and representations.\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/zE6Nr8trdrw/0.jpg)](https://www.youtube.com/watch?v=zE6Nr8trdrw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import YouTubeVideo\n",
    "# YouTubeVideo(\"zE6Nr8trdrw\",width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Excercise: Some questions for the video\n",
    ">\n",
    "> * Mention 10 examples of ways we can encode data\n",
    "> * Are all encoding created equally? Why not?\n",
    "> * Explain in your own words: What is the problem with pie-charts?\n",
    "> * Mention three encodings that are difficult for the human eye to parse? Can you find an example of a visualization online that uses one of those three?\n",
    "> * What is a \"Choropleth\"? What does the ancient Greek root of that word \"χώρος\" mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory data visualzation, two variables  \n",
    "\n",
    "I told you how I love the Data Analysis with Open Source Tools book. This time, we'll read Chapter 3, which is about visualizing data with two variables. \n",
    "\n",
    "*Reading*: DAOST Chapter 3 up to *Graphical Analysis and Presentation Graphics* on page 68 in the PDF. **You will have to go and get it on DTU Learn due to the copyright stuff**.\n",
    "\n",
    "And now a few exercises to reflect on the text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercises:* DAOST chapter 3\n",
    "\n",
    " > * Looking at Fig 3-1, Janert writes \"the data itself shows clearly that the amount of random noise in the data is small\". What do you think his argument is?\n",
    "> * Can you think of a real-world example of a multivariate relationship like the one in Fig 3-3 (lower right panel)?\n",
    "> * What are the two methods Janert metions for smoothing noisy data? Can you think of other ones?\n",
    "> * What is problematic about using straight lines to fit the data in Fig 3-5? (Something similar is actually the topic of a *Nature* article from 2004 get it [here](https://github.com/suneman/socialdataanalysis2018/blob/master/files/regrunners.pdf). And an extra [link](http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3366/pdf/imm3366.pdf) on this topic for the students who know Danish).\n",
    "> * What are residuals? Why is it a good idea to plot the residuals of your fit?\n",
    "> * Explain in your own words the point of the smooth tube in figure 3-7.\n",
    "> * What kind of relationships will a semi-log plot help you discover?\n",
    "> * What kind of functions will loglog plots help you see?\n",
    "> * What the h#ll is banking and what part of our visual system does it use to help us see patterns? What are potential problems with banking?\n",
    "> * I think figure 3-14 makes an important point about linear fits that is rarely made. What is it? \n",
    "> * Summarize the discussion of Graphical Analysis and Presentation Graphics on pp. 68-69 in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Logarithmic plots\n",
    "\n",
    "Ok. Let's briefly talk about logarithms and logarithimic plots (if you take my networks class in the fall semester you'll se lots of loglog plots since they're important for understanding a key property of networks).\n",
    "\n",
    "*Exercise*: Logarithms and plots\n",
    "\n",
    "> * First, we'll simply create a version of [this plot](https://raw.githubusercontent.com/suneman/socialdata2021/master/files/categories.png) from Lecture 1, where you display the $y$-axis on log-scale.\n",
    "> * Let's also try a loglog plot. Inspired by [this article](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0183110) I would expect that especially theft could be characterized by a power law distribution also in san francisco. Let's see if I'm right. \n",
    ">   - Step 1: Divide San Francisco into a grid **roughly** $100m \\times 100m$. You can, for example use numpy to do this, I would call `np.histogram2d`, and searching the internet, it seems that there are also [ways to do this in pandas](https://stackoverflow.com/questions/39254704/pandas-group-bins-of-data-per-longitude-latitude). The earth isn't flat,so `lat,` `lon` aren't really squares, but it is OK to ignore. \n",
    ">       * **Hint 1**. I really mean approximately 100 meters. It can also be 200 meters. Or 80 meters. Or 300.\n",
    ">       * **Hint 2**. Ignore outliers. We only want points that are on the San Fran peninsula\n",
    ">       * **Hint 3**. We've made a little example of how you can do the binning. Get it [here](https://github.com/suneman/socialdata2021/blob/main/lectures/week5_binning.ipynb).\n",
    ">   - Step 2: Count the number of thefts occurring within each grid-square (use all data for all time).\n",
    ">   - Step 3: Tally the counts. Count the number of squares with $k=0$ thefts. We call this $N(0)$. Next, count the number of grids with one crime to get $N(k=1)$. Keep going like this all the way up to $k=C_{max}$, where $C_{max}$ is the highest count of crimes you find in any grid space. \n",
    ">     * *Extra tip*: If you want all the details on binning for loglog axes, you can check out [Lecture 2, exercise 3](https://nbviewer.jupyter.org/github/SocialComplexityLab/socialgraphs2020/blob/master/lectures/Week2.ipynb?flush_cache=true) in my social graphs course.\n",
    ">   - Step 4: Plot the distribution of $k+1$ vs $N(k)$ on linear axes.\n",
    ">   - Step 5: Plot the distribution of $k+1$ vs $N(k)$ on loglog axes.\n",
    ">   - Step 6: Answer the question: Was Sune correct in assuming that there is a power-law distribution of theft?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Linear regression\n",
    "\n",
    "So now it's time for fun with standard linear regression. We'll get into that by asking the following question. \n",
    "\n",
    "> *Which pair of focus crimes have the the most similar temporal pattern across the week? (And which pair is most dissimilar).*\n",
    "\n",
    "Below I list the focus-crimes for your convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "focuscrimes = set(['WEAPON LAWS', 'PROSTITUTION', 'DRIVING UNDER THE INFLUENCE', 'ROBBERY', 'BURGLARY', 'ASSAULT', 'DRUNKENNESS', 'DRUG/NARCOTIC', 'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'VEHICLE THEFT', 'STOLEN PROPERTY', 'DISORDERLY CONDUCT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to answer this question is to plot the activity for all pairs of crimetypes as scatter plot per pair. One crime type on each axis, and where each point in the scatter corresponds to an hour of the week, and the number of crimes of crime-type 1 is on the $x$-axis and the number of crimes of crime-type 2 is on the $y$-axis. (So there will be 168 points in each scatterplot.)\n",
    "\n",
    "If we look at 14 focus crimes that results in\n",
    "$$\\frac{14\\times13}{2}=91$$\n",
    "\n",
    "pairwise comparisons. So we can display them all in a $7$ by $13$ matrix of plots. You can use matplotlib's `subplot` to organize those plots. With $7$ across and $13$ down, you should be able to squeeze them all onto a single [a4](https://en.wikipedia.org/wiki/ISO_216#A_series) page.\n",
    "\n",
    "*Exercise*:\n",
    "> Create the 91 scatterplots.\n",
    "> * Make sure to label each one with the two crime-types you're comparing so we can easily inspect visually.\n",
    "> * Make sure that that you squeeze the subplots closely together so each plot can be as big as possible. \n",
    "> * Just inspecting this matrix, which crime-types look correlated and which one look like they're very different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next it's time for the linear regression. Janert writes about this on page 63-66.\n",
    "\n",
    "There is a closed-form solution for linear regression. If you want to find the best straight line $y = ax + b$ fit to a set of $N$ points $\\{(x_1,y_1), (x_2,y_2), \\ldots (x_N,y_N)\\}$, the value of $b$ is\n",
    "\n",
    "$$\n",
    "\\tag{1}\n",
    "b = \\langle y \\rangle - a \\langle x \\rangle,\n",
    "$$\n",
    "\n",
    "where $\\langle x \\rangle = (1/N)\\sum_i x_i$ is the mean value of the $x_i$ and $\\langle y \\rangle = (1/N)\\sum_i x_i$ is the mean value of the $y_i$. \n",
    "\n",
    "And the value for the slope $a$ is \n",
    "\n",
    "$$\n",
    "\\tag{2}\n",
    "a = \\frac{\\sum_{i=1}^N \\left( x_iy_i \\right) - N \\langle x\\rangle\\langle y\\rangle }{\\sum_{i=1}^N\\left( x_i^2 \\right) - N\\langle x\\rangle^2}.\n",
    "$$\n",
    "\n",
    "Last year, I actually derived this whole thing. I've taken it out of the notebook. But if you'd like to take a look (it's a fun and instructive little exercise), you can find it **[here](https://github.com/suneman/socialdata2021/blob/main/lectures/LinearRegressionDerived.ipynb)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise*:\n",
    "> * Using the formulas we derived above (Equation 1 and 2), to calculate slopes for $a$ and $b$ in each case, and add a linear fit to each of the 91 scatterplots. (If you'd like, you can use a built package/function for fitting straight lines to check that everything is working as expected.)\n",
    "> * (Optional) You can add even more information to this plot by coloring each point according to its hour of the week. So create a gradient going from one color to another, and color each point according to the gradient. (So let's say your two colors are red and blue, then the Sunday, midnight to 1am bin will be red and the following Sunday, 11pm - midnight bin will be blue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness of fit as a measure of correlation\n",
    "\n",
    "But the question we started with was \"Which pair of focus crimes have the the most similar temporal pattern across the week?\". We haven't really answered that yet. So let's calculate one last thing: $R^2$. You probably also remember this one. \n",
    "\n",
    "Basically $R^2$ is a measurea of how good a linear fit is. You can [read about $R^2$ on wikipedia](https://en.wikipedia.org/wiki/Coefficient_of_determination). \n",
    "\n",
    "*Exercise:* \n",
    "> * Write a little function to calculate $R^2$ alongside each linear fit.\n",
    "> * **According to your fits and associated measure of $R^2$, which pair of crimes have the most similar temporal pattern. Discuss your finding: Does it make sense? Why?/Why not?**\n",
    "> * According to your fits and associated measure of $R^2$, which pair of crimes have the most **dis**similar temporal pattern. Discuss your finding: Does it make sense? Why?/Why not?\n",
    "> * Explain the connection between $R^2$ and the (Pearson correlation coefficient)[https://en.wikipedia.org/wiki/Pearson_correlation_coefficient].\n",
    "> * And speaking of correlations. In your words, explain the [Spearman correlation](https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient)? How is it different for the Pearson correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
